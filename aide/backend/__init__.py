from . import backend_anthropic, backend_openai, backend_openrouter, backend_gemini, backend_vllm
from .utils import FunctionSpec, OutputType, PromptType, compile_prompt_to_md
import re
import logging
import os

logger = logging.getLogger("aide")

VALID_PROVIDERS = {"openai", "anthropic", "openrouter", "gemini", "vllm"}


def _normalize_provider(provider: str | None) -> str | None:
    if not provider:
        return None
    provider = provider.strip().lower()
    if provider in VALID_PROVIDERS:
        return provider
    logger.warning("Unknown AIDE provider override: %s", provider)
    return None


def _get_forced_provider() -> str | None:
    return _normalize_provider(os.getenv("AIDE_PROVIDER") or os.getenv("AIDE_FORCE_PROVIDER"))


def _get_model_specific_provider(model: str) -> str | None:
    code_model = os.getenv("AIDE_CODE_MODEL")
    feedback_model = os.getenv("AIDE_FEEDBACK_MODEL")
    if code_model and model == code_model:
        return _normalize_provider(os.getenv("AIDE_CODE_PROVIDER"))
    if feedback_model and model == feedback_model:
        return _normalize_provider(os.getenv("AIDE_FEEDBACK_PROVIDER"))
    return None


def determine_provider(model: str) -> str:
    forced_provider = _get_forced_provider()
    if forced_provider:
        return forced_provider
    model_specific_provider = _get_model_specific_provider(model)
    if model_specific_provider:
        return model_specific_provider
    # Check if model matches OpenAI patterns first
    if re.match(r"^(gpt-.*|o\d+(-.*)?|codex-mini-latest)$", model):
        return "openai"
    elif model.startswith("claude-"):
        return "anthropic"
    elif model.startswith("gemini-"):
        return "gemini"
    # If a custom OpenAI-compatible base URL is set, use openai provider
    elif (
        os.getenv("OPENAI_BASE_URL")
        or os.getenv("OPENAI_API_BASE")
        or os.getenv("VLLM_API_BASE")
    ):
        return "openai"
    # all other models are handle by openrouter
    else:
        return "openrouter"


provider_to_query_func = {
    "openai": backend_openai.query,
    "anthropic": backend_anthropic.query,
    "openrouter": backend_openrouter.query,
    "gemini": backend_gemini.query,
    "vllm": backend_vllm.query,
}


def query(
    system_message: PromptType | None,
    user_message: PromptType | None,
    model: str,
    temperature: float | None = None,
    max_tokens: int | None = None,
    func_spec: FunctionSpec | None = None,
    **model_kwargs,
) -> OutputType:
    """
    General LLM query for various backends with a single system and user message.
    Supports function calling for some backends.

    Args:
        system_message (PromptType | None): Uncompiled system message (will generate a message following the OpenAI/Anthropic format)
        user_message (PromptType | None): Uncompiled user message (will generate a message following the OpenAI/Anthropic format)
        model (str): string identifier for the model to use (e.g. "gpt-4-turbo")
        temperature (float | None, optional): Temperature to sample at. Defaults to the model-specific default.
        max_tokens (int | None, optional): Maximum number of tokens to generate. Defaults to the model-specific max tokens.
        func_spec (FunctionSpec | None, optional): Optional FunctionSpec object defining a function call. If given, the return value will be a dict.

    Returns:
        OutputType: A string completion if func_spec is None, otherwise a dict with the function call details.
    """

    model_kwargs = model_kwargs | {
        "model": model,
        "temperature": temperature,
        "max_tokens": max_tokens,
    }

    provider = determine_provider(model)
    query_func = provider_to_query_func[provider]
    output, req_time, in_tok_count, out_tok_count, info = query_func(
        system_message=compile_prompt_to_md(system_message) if system_message else None,
        user_message=compile_prompt_to_md(user_message) if user_message else None,
        func_spec=func_spec,
        **model_kwargs,
    )

    return output
